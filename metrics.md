# Métriques de Classification en Deep Learning

### Exactitude (Accuracy)
- **Description**: Pourcentage des prédictions correctes parmi le total des prédictions.

### Précision (Precision)
- **Description**: Proportion des vraies prédictions positives parmi toutes les prédictions positives.

### Rappel (Recall)
- **Description**: Proportion des vraies prédictions positives parmi tous les échantillons positifs.

### F1-Score
- **Description**: Moyenne harmonique de la précision et du rappel, utilisée pour évaluer l'équilibre entre précision et rappel.

### Courbe ROC et AUC
- **Description**: La courbe ROC (Receiver Operating Characteristic) trace le taux de vrais positifs contre le taux de faux positifs. L'AUC (Area Under Curve) mesure l'aire sous la courbe ROC.
- **Interprétation**: Plus l'AUC est proche de 1, meilleure est la performance du modèle.

### Matrice de Confusion
- **Description**: Tableau récapitulatif des performances d'un modèle de classification, montrant les vrais positifs, faux positifs, vrais négatifs et faux négatifs.


### Courbe de Précision-Rappel (Precision-Recall Curve)
- **Description**: Trace la précision contre le rappel à différents seuils de classification.
- **Interprétation**: Utile pour évaluer la performance sur des classes déséquilibrées. Une grande surface sous la courbe indique une meilleure performance.


